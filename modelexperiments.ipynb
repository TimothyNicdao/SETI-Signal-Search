{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-16T21:34:19.303823Z","iopub.execute_input":"2021-07-16T21:34:19.304115Z","iopub.status.idle":"2021-07-16T21:34:19.314669Z","shell.execute_reply.started":"2021-07-16T21:34:19.304039Z","shell.execute_reply":"2021-07-16T21:34:19.31381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport math\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import DataLoader, Dataset\nfrom collections import Counter\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torchvision.transforms import ToTensor, Lambda, Compose\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection  import train_test_split\nfrom sklearn.model_selection import KFold\n\n\nfrom transformers import AutoModel, AutoTokenizer, BertTokenizer, AutoConfig\n\nimport itertools\nimport gc\nimport os \nimport random\n\nimport spacy\nimport time\nimport timeit","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:34:19.316082Z","iopub.execute_input":"2021-07-16T21:34:19.316607Z","iopub.status.idle":"2021-07-16T21:34:28.185172Z","shell.execute_reply.started":"2021-07-16T21:34:19.316564Z","shell.execute_reply":"2021-07-16T21:34:28.184275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(seed=1326)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:34:28.188604Z","iopub.execute_input":"2021-07-16T21:34:28.188883Z","iopub.status.idle":"2021-07-16T21:34:28.196856Z","shell.execute_reply.started":"2021-07-16T21:34:28.188857Z","shell.execute_reply":"2021-07-16T21:34:28.195971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HyperParameters","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# # for TPU\n# device = xm.xla_device()\n# torch.set_default_tensor_type('torch.FloatTensor')\n\nbatch_size = 256\nepochs = 4 # The number of epochs\n","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:34:28.198468Z","iopub.execute_input":"2021-07-16T21:34:28.198839Z","iopub.status.idle":"2021-07-16T21:34:28.24489Z","shell.execute_reply.started":"2021-07-16T21:34:28.198763Z","shell.execute_reply":"2021-07-16T21:34:28.243932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # load dataframe","metadata":{}},{"cell_type":"code","source":"\ntrain_path = '../input/seti-breakthrough-listen/train'\n\ntrain_labels = pd.read_csv('../input/seti-breakthrough-listen/train_labels.csv')\ntrain_labels","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:34:28.246415Z","iopub.execute_input":"2021-07-16T21:34:28.246763Z","iopub.status.idle":"2021-07-16T21:34:28.530134Z","shell.execute_reply.started":"2021-07-16T21:34:28.246721Z","shell.execute_reply":"2021-07-16T21:34:28.529392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image(filename, data_path=train_path):\n    data = np.load(f'{data_path}/{filename[0]}/{filename}.npy').astype(np.float32)\n    for i in range(data.shape[0]):\n        data[i] -= data[i].mean()\n        data[i] /= data[i].std()\n    return data\n\n\nres = load_image(train_labels.iloc[0][0])\nres.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:34:28.531472Z","iopub.execute_input":"2021-07-16T21:34:28.531807Z","iopub.status.idle":"2021-07-16T21:34:28.57643Z","shell.execute_reply.started":"2021-07-16T21:34:28.531772Z","shell.execute_reply":"2021-07-16T21:34:28.575707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image = load_image(self.df.iloc[idx][0])\n        label = self.df.iloc[idx][1]\n\n        return image, label\n        \n        \ntrain_data = ImageDataset(train_labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:34:28.578454Z","iopub.execute_input":"2021-07-16T21:34:28.578813Z","iopub.status.idle":"2021-07-16T21:34:28.586609Z","shell.execute_reply.started":"2021-07-16T21:34:28.578776Z","shell.execute_reply":"2021-07-16T21:34:28.585749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n","metadata":{}},{"cell_type":"code","source":"class SETIModel(nn.Module):\n    def __init__(self):\n        super(SETIModel,self).__init__()\n        self.dropout = torch.nn.Dropout(0.5)\n        self.LRelu = torch.nn.LeakyReLU(negative_slope=0.01)\n        self.Conv1 = torch.nn.Conv2d(6,6,5, stride = 2 )\n        self.Conv2 = torch.nn.Conv2d(6,3,5, stride = 2 )\n        self.Conv3 = torch.nn.Conv2d(3,3,5, stride = 2 )\n        self.linear1 = torch.nn.Linear(2697, 512)\n        self.linear2 = torch.nn.Linear(512, 2)\n\n\n                       \n    def forward(self,data):\n        x = self.Conv1(data)\n        x = self.dropout(x)\n        x = self.Conv2(x)\n        x = self.dropout(x)\n        x = self.Conv3(x)\n        x = torch.flatten(x, 1)\n        x = self.linear1(x)\n        x = self.LRelu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n#         print(f'x is {x.size()}' )\n        return x\n\n\n\n\nmodel = SETIModel().to(device)\ntorch.save(model.state_dict(), 'initialModel')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:34:28.58805Z","iopub.execute_input":"2021-07-16T21:34:28.588424Z","iopub.status.idle":"2021-07-16T21:34:34.450926Z","shell.execute_reply.started":"2021-07-16T21:34:28.58837Z","shell.execute_reply":"2021-07-16T21:34:34.450095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Eval","metadata":{}},{"cell_type":"code","source":"import time\n\nbptt = 35\n\ncriterion = nn.CrossEntropyLoss()\nlr = 0.0002 # learning rate\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95, verbose = True)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)\n\n\ndef train(model):\n    model.train() # Turn on the train mode\n    return_loss = []\n    total_loss = 0.\n    start_time = time.time()\n    batch = 0\n    for data, targets in train_loader:\n        model.train()\n        optimizer.zero_grad()\n      \n        data = torch.tensor(data).float().to(device)\n        final_output = model(data).to(device)\n        \n        targets = torch.tensor(targets).to(device)\n#         print(targets.size())\n        loss = criterion(final_output, targets)\n\n\n        loss.backward()\n#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n\n        total_loss += loss.item()\n        return_loss.append(loss.item())\n        log_interval = 100\n        batch += 1\n#         if batch % log_interval == 0 and batch > 0:\n#             cur_loss = total_loss / log_interval\n#             elapsed = time.time() - start_time\n#             print('| epoch {:3d} | {:5d}/{:5d} batches | '\n#                   'lr {:02.2f} | ms/batch {:5.2f} | '\n#                   'loss {:5.2f} | ppl {:8.2f}'.format(\n#                     epoch, batch, len(train_data) // bptt, scheduler.get_last_lr()[0],\n#                     elapsed * 1000 / log_interval,\n#                     cur_loss, np.exp(cur_loss)))\n        total_loss = 0\n        start_time = time.time()\n        \n    return torch.mean(torch.tensor(return_loss))\n\ndef evaluate(eval_model):\n#     losses = []\n    eval_model.eval() # Turn on the evaluation mode\n#     total_loss = 0.\n    total_loss = []\n    with torch.no_grad():\n        for data, targets in val_loader:\n            \n            data = torch.tensor(data).float().to(device)\n            final_output = model(data).to(device)\n            targets = torch.tensor(targets).to(device)\n            loss = criterion(final_output, targets)\n#             loss = criterion(final_output, targets)\n            currLoss = criterion(final_output, targets).item()\n#             total_loss += len(data) * currLoss\n            total_loss.append(currLoss)\n\n#     return total_loss\n    return torch.mean(torch.tensor(total_loss))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:34:34.452327Z","iopub.execute_input":"2021-07-16T21:34:34.452717Z","iopub.status.idle":"2021-07-16T21:34:34.46526Z","shell.execute_reply.started":"2021-07-16T21:34:34.452679Z","shell.execute_reply":"2021-07-16T21:34:34.464317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KFold Cross validation","metadata":{}},{"cell_type":"code","source":"# KFolds\n\nkfold = KFold(n_splits= 5 , shuffle=True)\n\n\nbest_val_loss = float(\"inf\")\nbest_model = None\nepoch_averages = [[] for _ in range(epochs)]\n\n\nfold_scores = []\n\nprint(f'length of dataset is {len(train_labels)}')\n\nfor fold, (train_ids, val_ids) in enumerate(kfold.split(train_labels)):\n    \n    # Print\n    print(f'FOLD {fold}')\n    print('--------------------------------')\n\n    # reset weights for the new fold\n    model.load_state_dict(torch.load('initialModel'))\n\n    \n    \n    # Sample elements randomly from a given list of ids, no replacement.\n    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n    \n    # Define data loaders for training and testing data in this fold\n    train_loader = DataLoader(train_data, batch_size = batch_size, sampler = train_subsampler, num_workers = 4)\n    val_loader = DataLoader(train_data, batch_size = batch_size, sampler = val_subsampler, num_workers = 4)\n    \n    \n    train_losses = []\n    val_losses = []\n    \n    # Run the training loop for defined number of epochs\n    for epoch in range(1, epochs +1):\n        # Print epoch\n        print(f'Starting epoch {epoch}')\n        epoch_start_time = time.time()\n\n\n        # Set current loss value\n        current_loss = 0.0\n\n        # Iterate over the DataLoader for training data\n        train_loss = train(model)\n        val_loss = evaluate(model)\n        \n        print('-' * 89)\n        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | train loss {:5.2f} |'.format(epoch, (time.time() - epoch_start_time),val_loss, train_loss))\n        print('-' * 89)\n\n#         if val_loss < best_val_loss:\n#             best_val_loss = val_loss\n#             best_model = model\n#             torch.save(best_model, 'bestModel')\n\n#         scheduler.step()\n        val_losses.append(val_loss)\n        train_losses.append(train_loss)\n        epoch_averages[epoch -1].append(val_loss)\n        if epoch % 5 == 0:\n            plt.plot(train_losses, label = \"Train_Loss\")\n            plt.plot(val_losses, label = \"Val_Loss\")\n            plt.show()\n            \n    fold_scores.append(np.mean(np.array(val_losses)))\n    scheduler.step()\n\n    \n#   # Print fold results\n#   print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n    print('--------------------------------')\n#   sum = 0.0\n#   for key, value in results.items():\n#     print(f'Fold {key}: {value} %')\n#     sum += value\n#   print(f'Average: {sum/len(results.items())} %')\n\n\nprint('*' * 89)\nprint('KFold has ended!!!!!!')\nprint(f'validation score is {np.mean(np.array(fold_scores))}')\nprint(f'rmse is {np.mean(np.array(fold_scores)) ** 0.5}')\nprint(len(fold_scores))\n\n\n\nbest_loss = float('inf')\nbest_epoch = 1\nfor epoch, values in enumerate(epoch_averages):\n    print(epoch + 1)\n    print(values)\n    print(np.mean(values))\n    if np.mean(values) < best_loss:\n        best_loss = np.mean(values)\n        best_epoch = epoch + 1\n        \nprint(f'The best average loss is {best_loss} with epoch: {best_epoch}')","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:34:34.466732Z","iopub.execute_input":"2021-07-16T21:34:34.46708Z","iopub.status.idle":"2021-07-16T22:15:19.244606Z","shell.execute_reply.started":"2021-07-16T21:34:34.467047Z","shell.execute_reply":"2021-07-16T22:15:19.241326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True, num_workers = 4)\n\n\nmodel.load_state_dict(torch.load('initialModel'))\n\nfor epoch in range(1, best_epoch + 1):\n        print('-' * 89)\n        print(f'Starting epoch {epoch}')\n        epoch_start_time = time.time()\n        train_loss = train(model)\n        print(f'| end of epoch: {epoch} | time: {time.time() - epoch_start_time}s  | train loss: {train_loss} |')\n        print('-' * 89)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:15:19.247009Z","iopub.status.idle":"2021-07-16T22:15:19.247576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'bestModel')","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:15:19.248927Z","iopub.status.idle":"2021-07-16T22:15:19.249519Z"},"trusted":true},"execution_count":null,"outputs":[]}]}